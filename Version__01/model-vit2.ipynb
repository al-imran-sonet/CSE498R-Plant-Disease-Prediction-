{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13166744,"sourceType":"datasetVersion","datasetId":8343159}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, time, random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport timm\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T16:41:42.480292Z","iopub.execute_input":"2025-10-10T16:41:42.480774Z","iopub.status.idle":"2025-10-10T16:41:42.484912Z","shell.execute_reply.started":"2025-10-10T16:41:42.480749Z","shell.execute_reply":"2025-10-10T16:41:42.484205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CONFIGURATION\n\nDATA_ROOT = \"/kaggle/input/plant-disease-dataset/Dataset_Final_V2_Split\"  # change to your dataset path\nTRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\nVAL_DIR   = os.path.join(DATA_ROOT, \"val\")\nTEST_DIR  = os.path.join(DATA_ROOT, \"test\")\n\nBATCH_SIZE = 128\nIMG_SIZE = 224\nEPOCHS = 50\nLR = 3e-5\nWEIGHT_DECAY = 1e-4\nPATIENCE = 5\nNUM_WORKERS = 4\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nMODEL_NAME = \"vit_tiny_patch16_224\"  \nUSE_PRETRAINED = True\nOUTPUT_DIR = \"/kaggle/working\"\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T16:47:43.534022Z","iopub.execute_input":"2025-10-10T16:47:43.534316Z","iopub.status.idle":"2025-10-10T16:47:43.541495Z","shell.execute_reply.started":"2025-10-10T16:47:43.534296Z","shell.execute_reply":"2025-10-10T16:47:43.540867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DATASETS & DATALOADERS\n\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    normalize\n])\n\ntrain_dataset = datasets.ImageFolder(TRAIN_DIR, transform=transform)\nvalid_dataset = datasets.ImageFolder(VAL_DIR, transform=transform)\ntest_dataset  = datasets.ImageFolder(TEST_DIR, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\ntest_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\nclass_names = train_dataset.classes\nnum_classes = len(class_names)\n\nprint(f\"‚úÖ Data loaded: {len(train_dataset)} train, {len(valid_dataset)} valid, {len(test_dataset)} test images\")\nprint(f\"Classes: {num_classes} ‚Üí {class_names}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T16:47:47.947319Z","iopub.execute_input":"2025-10-10T16:47:47.947596Z","iopub.status.idle":"2025-10-10T16:48:52.261953Z","shell.execute_reply.started":"2025-10-10T16:47:47.947573Z","shell.execute_reply":"2025-10-10T16:48:52.261247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# MODEL\n\nprint(f\"\\nüîß Creating model: {MODEL_NAME}\")\nmodel = timm.create_model(MODEL_NAME, pretrained=USE_PRETRAINED, num_classes=num_classes)\nmodel = model.to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T16:50:17.299813Z","iopub.execute_input":"2025-10-10T16:50:17.300478Z","iopub.status.idle":"2025-10-10T16:50:17.662810Z","shell.execute_reply.started":"2025-10-10T16:50:17.300440Z","shell.execute_reply":"2025-10-10T16:50:17.662017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# TRAINING LOOP\n\nbest_val_acc = 0.0\nearly_stop_counter = 0\nhistory = []\n\ndef accuracy(output, target):\n    _, preds = torch.max(output, 1)\n    return (preds == target).sum().item()\n\nfor epoch in range(1, EPOCHS + 1):\n    start_time = time.time()\n    model.train()\n\n    train_loss, train_correct, train_total = 0, 0, 0\n    val_loss, val_correct, val_total = 0, 0, 0\n\n    # ---- Training ----\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Train]\", leave=False)\n    for inputs, labels in train_bar:\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item() * inputs.size(0)\n        train_correct += accuracy(outputs, labels)\n        train_total += labels.size(0)\n        progress = 100 * train_total / len(train_loader.dataset)\n        train_bar.set_postfix(loss=loss.item(), progress=f\"{progress:.1f}%\")\n\n    avg_train_loss = train_loss / train_total\n    train_acc = 100 * train_correct / train_total\n    # ---- Validation ----\n    model.eval()\n    with torch.no_grad():\n        val_bar = tqdm(valid_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Val]\", leave=False)\n        for inputs, labels in val_bar:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            val_correct += accuracy(outputs, labels)\n            val_total += labels.size(0)\n            progress = 100 * val_total / len(valid_loader.dataset)\n            val_bar.set_postfix(loss=loss.item(), progress=f\"{progress:.1f}%\")\n\n    avg_val_loss = val_loss / val_total\n    val_acc = 100 * val_correct / val_total\n\n    scheduler.step()\n    epoch_time = time.time() - start_time\n\n    print(f\"Epoch {epoch:03}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}% | Time: {epoch_time:.1f}s\")\n\n    history.append({\n        \"Epoch\": epoch,\n        \"Train Loss\": avg_train_loss,\n        \"Train Acc\": train_acc,\n        \"Val Loss\": avg_val_loss,\n        \"Val Acc\": val_acc,\n        \"Time (s)\": epoch_time\n    })\n    pd.DataFrame(history).to_csv(os.path.join(OUTPUT_DIR, \"vitv2_history.csv\"), index=False)\n\n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        early_stop_counter = 0\n        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_vitv2_model.pth\"))\n        print(f\"üü¢ Saved new best model (Val Acc: {best_val_acc:.2f}%)\")\n    else:\n        early_stop_counter += 1\n        print(f\"‚è∏ Early stop counter: {early_stop_counter}/{PATIENCE}\")\n        if early_stop_counter >= PATIENCE:\n            print(\"‚ö†Ô∏è Early stopping triggered.\")\n            break\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T16:50:43.876208Z","iopub.execute_input":"2025-10-10T16:50:43.876890Z","iopub.status.idle":"2025-10-10T18:09:45.358829Z","shell.execute_reply.started":"2025-10-10T16:50:43.876867Z","shell.execute_reply":"2025-10-10T18:09:45.357765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TEST EVALUATION\n\nprint(\"\\nüîç Evaluating best model on test set...\")\nmodel.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, \"best_vitv2_model.pth\")))\nmodel.eval()\n\ntest_loss, test_correct, test_total = 0, 0, 0\nall_labels, all_preds = [], []\n\nwith torch.no_grad():\n    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        test_correct += (preds == labels).sum().item()\n        test_total += labels.size(0)\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(preds.cpu().numpy())\n\ntest_loss /= test_total\ntest_acc = 100 * test_correct / test_total\nprint(f\"‚úÖ Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T18:12:56.078270Z","iopub.execute_input":"2025-10-10T18:12:56.078957Z","iopub.status.idle":"2025-10-10T18:13:45.484117Z","shell.execute_reply.started":"2025-10-10T18:12:56.078919Z","shell.execute_reply":"2025-10-10T18:13:45.483166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# CONFUSION MATRIX & REPORT\n\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\nfig, ax = plt.subplots(figsize=(10, 10))\ndisp.plot(cmap=\"Blues\", ax=ax, xticks_rotation=90)\nplt.title(\"Confusion Matrix - ViT v2\")\nplt.show()\n\nprint(\"\\nüìä Classification Report:\\n\")\nprint(classification_report(all_labels, all_preds, target_names=class_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T18:13:52.875361Z","iopub.execute_input":"2025-10-10T18:13:52.876025Z","iopub.status.idle":"2025-10-10T18:13:56.859186Z","shell.execute_reply.started":"2025-10-10T18:13:52.875987Z","shell.execute_reply":"2025-10-10T18:13:56.858339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TRAINING CURVES\n\nhist_df = pd.read_csv(os.path.join(OUTPUT_DIR, \"vitv2_history.csv\"))\n\nplt.figure(figsize=(10,5))\nplt.plot(hist_df[\"Epoch\"], hist_df[\"Train Acc\"], label=\"Train Acc\")\nplt.plot(hist_df[\"Epoch\"], hist_df[\"Val Acc\"], label=\"Val Acc\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\")\nplt.title(\"Training & Validation Accuracy (ViT v2)\")\nplt.legend(); plt.grid(); plt.show()\n\nplt.figure(figsize=(10,5))\nplt.plot(hist_df[\"Epoch\"], hist_df[\"Train Loss\"], label=\"Train Loss\")\nplt.plot(hist_df[\"Epoch\"], hist_df[\"Val Loss\"], label=\"Val Loss\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\nplt.title(\"Training & Validation Loss (ViT v2)\")\nplt.legend(); plt.grid(); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T18:14:05.686399Z","iopub.execute_input":"2025-10-10T18:14:05.686696Z","iopub.status.idle":"2025-10-10T18:14:06.041836Z","shell.execute_reply.started":"2025-10-10T18:14:05.686676Z","shell.execute_reply":"2025-10-10T18:14:06.041092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}